# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1939N4frUvPKj97HTyBvVwqMdLphcLhkX
"""

# Load essential libraries for data preprocessing, visualization, and modeling
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

from google.colab import files
uploaded = files.upload()

data = pd.read_csv("AmesHousing.csv")

# Display the first few rows
data.head()

# Step 2: Identify the target variable and independent variables
# Target (dependent variable): SalePrice
# Features (independent variables): All other columns except 'SalePrice'
print("Shape of dataset:", data.shape)
print("\nColumns in dataset:\n", data.columns)

# Step 3: Exploratory Data Analysis - correlation matrix
plt.figure(figsize=(14, 10))
correlation = data.corr(numeric_only=True)
sns.heatmap(correlation[['SalePrice']].sort_values(by='SalePrice', ascending=False), annot=True, cmap='coolwarm')
plt.title("Correlation of features with SalePrice")
plt.show()

# Select features with high correlation to SalePrice for modeling
top_corr_features = correlation['SalePrice'].abs().sort_values(ascending=False).head(10).index.tolist()
top_corr_features.remove('SalePrice')

# Step 4: Prepare data for model
X = data[top_corr_features]
y = data['SalePrice']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Preprocessing pipeline
# Identify numerical and categorical features
numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()

# Define preprocessing for numeric columns (impute + scale)
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Define preprocessing for categorical columns (impute + one-hot)
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine both transformers into a ColumnTransformer
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# Step 6: Build a Random Forest model inside a pipeline
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
])

# Step 7: Train the model
model.fit(X_train, y_train)

# Step 8: Predict and evaluate the model
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"Root Mean Squared Error (RMSE) on Test Data: {rmse:.2f}")

# Step 9: Feature importance from the Random Forest
# Only numeric features are used in this simple example
importances = model.named_steps['regressor'].feature_importances_

# If one-hot features exist, get their names
try:
    ohe_columns = model.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features)
    feature_names = numeric_features + ohe_columns.tolist()
except:
    feature_names = numeric_features

# Plot feature importances
feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False).head(10)

plt.figure(figsize=(10, 6))
sns.barplot(data=feature_importance_df, x='importance', y='feature')
plt.title("Top 10 Important Features for Predicting House Prices")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.show()